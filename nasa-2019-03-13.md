Dask: Scaling Science in Python
-------------------------------

<img src="images/dask_icon.svg" width=20%>

*Matthew Rocklin*

NVIDIA



-  Jupyter Notebook/Script/...
-  NumPy/Pandas/Scikit-Learn/...
-  ...
-  ...
-  ...
-  Cluster of hardware

<hr>

Dask mediates between Python users and distributed hardware


-  Jupyter Notebook/Script/...
-  NumPy/Pandas/Scikit-Learn/...
-  ...
-  ... MPI, Spark, Dask, ...
-  ...
-  Cluster of hardware

<hr>

Dask mediates between Python users and distributed hardware


-  Jupyter Notebook/Script/...
-  NumPy/Pandas/Scikit-Learn/...
-  **Parallel algorithms**
-  **Distributed execution**
-  **Sensible deployment**
-  Cluster of hardware

<hr>

Dask mediates between Python users and distributed hardware



-  Jupyter Notebook/Script/...
-  NumPy/Pandas/Scikit-Learn/...
-  **Parallel algorithms**
-  Distributed execution
-  Sensible deployment
-  Cluster of hardware

<hr>

Dask mediates between Python users and distributed hardware


### High Level: Dask scales other Python libraries

-  Pandas

    ```python
    df = pandas.read_csv('my-file.csv')

    df.groupby(df.timestamp.dt.hour).value.mean()
    ```

-  Numpy

    ```python
    X = numpy.random.random((1000, 1000))

    (X + X.T) - X.mean(axis=0)
    ```

-  Scikit-Learn

    ```python
    from scikit_learn.linear_models import LogisticRegression

    model = LogisticRegression()
    model.fit(X, y)
    ```

-  ... and several other applications throughout PyData


### High Level: Dask scales other Python libraries

-  Pandas + Dask

    ```python
    df = dask.dataframe.read_csv('s3://path/to/*.csv')

    df.groupby(df.timestamp.dt.hour).value.mean()
    ```

-  Numpy + Dask

    ```python
    X = dask.array.random((100000, 100000), chunks="1 GiB")

    (X + X.T) - X.mean(axis=0)
    ```

-  Scikit-Learn + Dask + ...

    ```python
    from dask_ml.linear_models import LogisticRegression

    model = LogisticRegression()
    model.fit(X, y)
    ```

-  ... and several other applications throughout PyData


### Dask.DataFrame

<img src="images/dask-dataframe-inverted.svg" width="30%">

    import pandas as pd
    df = pd.read_csv('myfile.csv', parse_dates=['timestamp'])
    df.groupby(df.timestamp.dt.hour).value.mean()

    import dask.dataframe as dd
    df = dd.read_csv('s3://myfiles.*.csv', parse_dates=['timestamp'])
    df.groupby(df.timestamp.dt.hour).value.mean()


### Dask.array

<img src="images/dask-array.svg" width="60%">

    import numpy as np
    x = np.random.random((1000, 1000))
    y = x + x.T - x.mean(axis=0)

    import dask.array as da
    x = da.random.random((100000, 100000), chunks="1 GiB")
    y = x + x.T - x.mean(axis=0)


### Fine Grained Python Code

    .

<hr>

    results = {}
    .
    .

    for a in A:
        for b in B:
            if a < b:
                results[a, b] = f(a, b)
            else:
                results[a, b] = g(a, b)

    .


### Fine Grained Python Code

    import dask

<hr>

    results = {}
    f = dask.delayed(f)  # mark functions as lazily evaluated
    g = dask.delayed(g)

    for a in A:
        for b in B:
            if a < b:
                results[a, b] = f(a, b)  # construct task graph
            else:
                results[a, b] = g(a, b)

    results = dask.compute(results)  # trigger computation



-  Jupyter Notebook/Script/...
-  NumPy/Pandas/Scikit-Learn/...
-  **Parallel algorithms**
-  Distributed execution
-  Sensible deployment
-  Cluster of hardware

<hr>

Dask mediates between Python users and distributed hardware


-  Jupyter Notebook/Script/...
-  NumPy/Pandas/Scikit-Learn/...
-  Parallel algorithms
-  **Distributed execution**
-  Sensible deployment
-  Cluster of hardware

<hr>

Dask mediates between Python users and distributed hardware


### Dask is a task scheduler

Like `make`, but where each task is a short Python function

    (X + X.T) - X.mean(axis=0)  # Dask code turns into task graphs

<img src="images/grid_search_schedule-0.png" width="100%">


### Dask is a task scheduler

Like `make`, but where each task is a short Python function

    (X + X.T) - X.mean(axis=0)  # Dask code turns into task graphs

<img src="images/grid_search_schedule.gif" width="100%">


### Dask thinks about ...

-  Data locality
-  Load balancing
-  Scarce resources
-  Network communication
-  Resilience
-  Scaling up and down
-  Diagnostics
-  Profiling
-  ...



-  Jupyter Notebook/Script/...
-  NumPy/Pandas/Scikit-Learn/...
-  Parallel algorithms
-  **Distributed execution**
-  Sensible deployment
-  Cluster of hardware

<hr>

Dask mediates between Python users and distributed hardware


-  Jupyter Notebook/Script/...
-  NumPy/Pandas/Scikit-Learn/...
-  Parallel algorithms
-  Distributed execution
-  **Sensible deployment**
-  Cluster of hardware

<hr>

Dask mediates between Python users and distributed hardware


### Deploys on Standard Cluster Hardware

<img src="images/network-inverse.svg" width="50%" align="right">

-  HPC:
    -  SLURM
    -  PBS
    -  SGE
    -  LSF
    -  ...
-   Cloud with Kubernetes
-   Hadoop/Spark with Yarn


### Deploys on Standard Cluster Hardware

```python
>>> from dask_jobqueue import SLURMCluster
>>> cluster = SLURMCluster(cores=24,
                           memory="100GB",
                           project="my-project",
                           queue="regular")

>>> cluster.scale(10)  # ask for ten nodes

>>> cluster.adapt(minimum=0, maximum=100)  # or adapt nodes based on load
```

Integrates with widely deployed HPC job schedulers


### Deploys on Standard Cluster Hardware

```python
>>> from dask_kubenetes import KubeCluster
>>> cluster = KubeCluster(...,
                          ...,
                          ...,
                          ...)

>>> cluster.scale(10)  # ask for ten nodes

>>> cluster.adapt(minimum=0, maximum=100)  # or adapt nodes based on load
```

Or newer cloud technologies


### Deploys on Standard Cluster Hardware

```python
>>> from dask_yarn import YarnCluster
>>> cluster = YarnCluster(...,
                          ...,
                          ...,
                          ...)

>>> cluster.scale(10)  # ask for ten nodes

>>> cluster.adapt(minimum=0, maximum=100)  # or adapt nodes based on load
```

Or Hadoop clusters you may have


### Deploys on Standard Cluster Hardware

```python
>>> from dask.distributed import LocalCluster
>>> cluster = LocalCluster()
.
.
.
.
.
.
.
```

Or, most commonly, a laptop



### GPU Integration

-  Dask scales NumPy, Pandas, ...
-  Dask scales CuPy, RAPIDS, ...


### Learn More

<img src="images/dask_icon.svg" width=20%>

[pangeo.io](https://pangeo.io)

[dask.org](https://dask.org)

[examples.dask.org](https://examples.dask.org)
